Namespace(batch_size=32, data_dir='/root/dataset/Osteoporosis/Sagittal_Contrast_Final_1/', epochs=60, log='logs/CE', lr=0.0001, n_gpu=4, num_classes=3, phase='test', seed=1, weight_decay=0.001, workers=16)
multi_class.py:43: UserWarning: You have chosen to seed training. This will turn on the CUDNN deterministic setting, which can slow down your training considerably! You may see unexpected behavior when restarting from checkpoints.
  warnings.warn('You have chosen to seed training. '
Traceback (most recent call last):
  File "multi_class.py", line 251, in <module>
    main(args)
  File "multi_class.py", line 90, in main
    acc1 = validate(model, val_loader, criterion, device, args, True)
  File "multi_class.py", line 187, in validate
    output = model(data)
  File "/root/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/root/anaconda3/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 157, in forward
    inputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)
  File "/root/anaconda3/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 168, in scatter
    return scatter_kwargs(inputs, kwargs, device_ids, dim=self.dim)
  File "/root/anaconda3/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py", line 36, in scatter_kwargs
    inputs = scatter(inputs, target_gpus, dim) if inputs else []
  File "/root/anaconda3/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py", line 28, in scatter
    res = scatter_map(inputs)
  File "/root/anaconda3/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py", line 15, in scatter_map
    return list(zip(*map(scatter_map, obj)))
  File "/root/anaconda3/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py", line 13, in scatter_map
    return Scatter.apply(target_gpus, None, dim, obj)
  File "/root/anaconda3/lib/python3.8/site-packages/torch/nn/parallel/_functions.py", line 92, in forward
    outputs = comm.scatter(input, target_gpus, chunk_sizes, ctx.dim, streams)
  File "/root/anaconda3/lib/python3.8/site-packages/torch/nn/parallel/comm.py", line 186, in scatter
    return tuple(torch._C._scatter(tensor, devices, chunk_sizes, dim, streams))
KeyboardInterrupt
